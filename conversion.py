import modal
import subprocess
import os
import shutil

# --- 1. Define Image (Added Build Tools) ---
image = (
    modal.Image.debian_slim()
    .apt_install(
        "git", "wget", "unzip",
        "cmake", "pkg-config", "build-essential"  # Required for gguf/sentencepiece compilation
    )
    .pip_install(
        "torch",
        "numpy",
        "sentencepiece",
        "transformers",
        "accelerate"
    )
    # Add your modified script (the one with the MLP Fix)
    .add_local_file("convert_hf_to_gguf.py", "/root/convert_hf_to_gguf.py")
)

# CHANGED: Connect to the volume created by your pruning script
vol = modal.Volume.from_name("llama31-mlp-only") 
app = modal.App("llama-folder-converter")

@app.function(
    image=image,
    volumes={"/data": vol},
    timeout=7200,
    gpu="T4",
)
def run_folder_conversion():
    import torch
    import transformers
    
    # --- CONFIGURATION ---
    # CHANGED: Point to the location where pruning.py saved the logs
    # Structure from pruning.py: /outputs/prune_log/llama31_mlp_only
    # Mounted here as:           /data/prune_log/llama31_mlp_only
    RAW_DIR = "/data/prune_log/llama31_mlp_only"
    
    # Output paths (will be created in the same volume)
    CLEAN_DIR = "/data/clean_model"
    GGUF_FILE = "/data/pruned_model.gguf"
    
    # We clone into a specific folder to keep things clean
    REPO_DIR = "/root/llama-repo"
    
    os.makedirs(CLEAN_DIR, exist_ok=True)

    # --- 1. VERIFY RAW FILES (No Download Needed) ---
    print(f"üîç Checking for pruned model at: {RAW_DIR}")
    if not os.path.exists(f"{RAW_DIR}/pytorch_model.bin"):
        print("‚ùå Error: pruned_model.bin not found!")
        print("   Did the pruning script finish successfully?")
        print("   Current volume contents:")
        # Helper to debug path issues
        for root, dirs, files in os.walk("/data"):
            print(f"   {root}/ : {files}")
        raise FileNotFoundError("Raw checkpoint not found in volume.")
    else:
        print("‚úÖ Found raw checkpoint from pruning script.")

    # --- 2. CLEAN CHECKPOINT ---
    if not os.path.exists(f"{CLEAN_DIR}/config.json"):
        print("üßπ Cleaning LLM-Pruner checkpoint...")
        checkpoint_path = f"{RAW_DIR}/pytorch_model.bin"
        
        try:
            print(f"   Loading custom checkpoint (Unpickling)...")
            
            # FIX 1: weights_only=False allows loading the custom object
            ckpt = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
            
            if 'model' in ckpt:
                print("   Found 'model' key. Extracting...")
                model_obj = ckpt['model']
                print(f"   Saving standard HF format to {CLEAN_DIR}...")
                model_obj.save_pretrained(CLEAN_DIR)
                del model_obj
            else:
                print("‚ùå 'model' key not found! Keys found:", ckpt.keys())
                raise Exception("Invalid Checkpoint Format")

            if 'tokenizer' in ckpt:
                print("   Found 'tokenizer' key. Saving...")
                ckpt['tokenizer'].save_pretrained(CLEAN_DIR)
            else:
                print("‚ö†Ô∏è No tokenizer found in checkpoint. Checking folder artifacts...")
                # Fallback: Copy tokenizer files generated by pruning script if they exist
                files_copied = 0
                for f in os.listdir(RAW_DIR):
                    if "token" in f or "vocab" in f:
                        shutil.copy(f"{RAW_DIR}/{f}", f"{CLEAN_DIR}/{f}")
                        files_copied += 1
                if files_copied > 0:
                    print(f"   ‚úÖ Copied {files_copied} tokenizer files manually.")
                else:
                    print("   ‚ö†Ô∏è  Warning: No tokenizer files found. GGUF conversion might fail.")

            del ckpt
            print("‚úÖ Cleaning complete.")
            
        except Exception as e:
            print(f"‚ùå Failed to clean checkpoint: {e}")
            raise
    else:
        print("‚úÖ Cleaned model already exists.")

    # --- 3. PREPARE ENVIRONMENT ---
    if not os.path.exists(REPO_DIR):
        print("üì• Cloning SPECIFIC llama.cpp fork (ymcki)...")
        subprocess.run(["git", "clone", "https://github.com/ymcki/llama.cpp-b4139", REPO_DIR], check=True)
        
        print("üîß Installing MATCHING gguf library from source...")
        subprocess.run(["pip", "install", f"{REPO_DIR}/gguf-py"], check=True)

    # --- 4. RUN CONVERSION ---
    print("üöÄ Running GGUF conversion...")
    
    # We run your LOCALLY mounted script (which has the MLP fix), 
    # but using the environment (gguf lib) we just installed from the repo.
    convert_cmd = [
        "python3", "/root/convert_hf_to_gguf.py",
        CLEAN_DIR,
        "--outfile", GGUF_FILE,
        "--outtype", "f16"
    ]
    
    result = subprocess.run(convert_cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print("‚ùå Conversion Failed!")
        print(result.stderr)
        raise Exception("Conversion script failed")
    
    print(result.stdout)
    print(f"‚úÖ Conversion complete! File saved at: {GGUF_FILE}")
    vol.commit()

@app.local_entrypoint()
def main():
    run_folder_conversion.remote()
